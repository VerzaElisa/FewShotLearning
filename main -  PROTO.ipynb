{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e2b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 16:22:34.648658: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 16:22:34.694677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-07 16:22:35.699699: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from prototypical.train.train_setup import train\n",
    "\n",
    "DATA_DIR = os.path.join('data', 'mammals_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1e4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759854160.667909  194066 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18314 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:18:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 splits with 10 classes each.\n",
      "Training started.\n",
      "Epoch 1 started.\n",
      "support shape: (4, 2, 164, 397, 3), query shape: (4, 3, 164, 397, 3)\n",
      "z_prototypes shape: (4, 15360), z_query shape: (12, 15360)\n",
      "support shape: (4, 2, 164, 397, 3), query shape: (4, 3, 164, 397, 3)\n",
      "z_prototypes shape: (4, 15360), z_query shape: (12, 15360)\n",
      "support shape: (4, 2, 164, 397, 3), query shape: (4, 3, 164, 397, 3)\n",
      "z_prototypes shape: (4, 15360), z_query shape: (12, 15360)\n",
      "support shape: (4, 2, 164, 397, 3), query shape: (4, 3, 164, 397, 3)\n",
      "z_prototypes shape: (4, 15360), z_query shape: (12, 15360)\n",
      "Episode 2\n",
      "Episode 4\n",
      "Episode 6\n",
      "Episode 8\n",
      "Episode 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ended.\n",
      "Epoch 2, Loss: 1.3858360052108765, Accuracy: 44.16666793823242, Val Loss: 1.383247971534729, Val Accuracy: 56.296302795410156\n",
      "Saving new best model with loss:  1.383248\n",
      "Epoch 2 started.\n",
      "Episode 10\n",
      "Episode 12\n",
      "Episode 14\n",
      "Episode 16\n",
      "Episode 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 ended.\n",
      "Epoch 3, Loss: 1.3790576457977295, Accuracy: 43.33333206176758, Val Loss: 1.2523112297058105, Val Accuracy: 54.35184860229492\n",
      "Saving new best model with loss:  1.2523112\n",
      "Training ended.\n",
      "Training succeed!\n",
      "Training took: 0.0 h 0.0 min 18.617745876312256 sec\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"data.dataset\": \"mammals_calls\",\n",
    "    \"data.split\": \"custom\",\n",
    "    \"data.train_way\": 4,\n",
    "    \"data.train_support\": 2,\n",
    "    \"data.train_query\": 3,\n",
    "    \"data.test_way\": 4,\n",
    "    \"data.test_support\": 2,\n",
    "    \"data.test_query\": 3,\n",
    "    \"data.episodes\": 10,\n",
    "    \"data.gpu\": 0,\n",
    "    \"data.cuda\":False,\n",
    "    \"model.x_dim\": \"164,397,3\",\n",
    "    \"model.z_dim\": 64,\n",
    "    \"train.epochs\": 2,\n",
    "    'train.optim_method': \"Adam\",\n",
    "    \"train.lr\": 0.001,\n",
    "    \"train.patience\": 5,\n",
    "    \"model.save_path\": 'data_cache/proto/test_mammals_calls.h5'\n",
    "}\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58809020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  split  class_len\n",
      "0  data  train         52 52\n"
     ]
    }
   ],
   "source": [
    "class_split = os.listdir(DATA_DIR)\n",
    "split_list = []\n",
    "split_dict = {}\n",
    "sum = 0\n",
    "\n",
    "for dir in class_split:\n",
    "    dir_len = len(os.listdir(os.path.join(DATA_DIR, dir)))\n",
    "    if dir_len > 50 and dir_len < 500:\n",
    "        split_list.append((dir, dir_len))\n",
    "        sum += dir_len\n",
    "train_len = len(split_list)/2\n",
    "len_list = []\n",
    "for i in range(len(split_list)):\n",
    "    split = 'train' if i < train_len else 'val'\n",
    "    split_dict[split_list[i][0]] = split\n",
    "    len_list.append(split_list[i][1])\n",
    "\n",
    "split_df = pd.DataFrame(list(split_dict.items()), columns=['class', 'split'])\n",
    "split_df['class_len'] = len_list\n",
    "print(split_df, sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
