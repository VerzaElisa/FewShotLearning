{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22efa032",
   "metadata": {},
   "source": [
    "## Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfa9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from CNN.utility import train_routine\n",
    "from CNN.loader import load_dataset, get_split\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "SPLIT_PERC = {'train': 0.8, 'val': 0.2}\n",
    "DATA_DIR = os.path.join('data', 'mammals_calls')\n",
    "AUDIO_DIR = os.path.join('data', 'audio')\n",
    "TO_TRAIN = False\n",
    "h = 164\n",
    "w = 397\n",
    "\n",
    "seed = 2025\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88298a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = [f.path for f in os.scandir(DATA_DIR) if f.is_dir()]\n",
    "data_info = {}\n",
    "for subfolder in subfolders:\n",
    "    species_name = os.path.basename(subfolder)\n",
    "    file_count = len([f for f in os.listdir(subfolder) if os.path.isfile(os.path.join(subfolder, f))])\n",
    "    data_info[species_name] = file_count\n",
    "count_df = pd.DataFrame(list(data_info.items()), columns=['species', 'file_count'])\n",
    "count_df = count_df.sort_values(by='file_count', ascending=False)\n",
    "print(count_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_thousands = count_df[count_df['file_count'] > 1000]\n",
    "count_hundreds = count_df[(count_df['file_count'] > 100) & (count_df['file_count'] < 1000)]\n",
    "count_tens = count_df[count_df['file_count'] < 100]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(count_thousands['species'], count_thousands['file_count'], color=\"#87CEEB\")\n",
    "plt.bar(count_hundreds['species'], count_hundreds['file_count'], color='#00688B')\n",
    "plt.bar(count_tens['species'], count_tens['file_count'], color=\"#191970\")\n",
    "plt.xlabel('Species')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Log Scaled Number of Spectrograms')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Number of Spectrograms per Species')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Total number of species: {count_df.shape[0]}')\n",
    "print(count_thousands)\n",
    "print(f'Number of species with more than 1000 spectrograms: {count_thousands.shape[0]}')\n",
    "print(f'Number of species with more than 100 but less than 1000 spectrograms: {count_hundreds.shape[0]}')\n",
    "print(f'Number of species with less than 100 spectrograms: {count_tens.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_extractor(row, chunk_size):\n",
    "    try:\n",
    "        signal, sr = librosa.load(row['audio_files'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {row['audio_files']}: {e}\")\n",
    "        row['chunk_list'] = []\n",
    "        return row\n",
    "    chunk_size = chunk_size * sr\n",
    "    mfcc_chunks = []\n",
    "    i = 1\n",
    "    \n",
    "    for start in range(0, len(signal), sr):\n",
    "        i += 1\n",
    "        end = start + chunk_size\n",
    "        y_chunk = signal[start:end]\n",
    "        \n",
    "        if len(y_chunk) < chunk_size:\n",
    "            break  \n",
    "        mfcc = librosa.feature.mfcc(y=y_chunk, sr=sr, n_mfcc=50)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "\n",
    "        mfcc_chunks.append(mfcc_mean)\n",
    "    row['chunk_list'] = mfcc_chunks\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1700\n",
    "audio_files = {}\n",
    "for species in count_thousands['species']:\n",
    "    curr_path = os.path.join(AUDIO_DIR, species)\n",
    "    audio_files[species] = [os.path.join(curr_path, f) for f in os.listdir(curr_path) if f.endswith('.wav')]\n",
    "\n",
    "audio_df = pd.DataFrame(list(audio_files.items()), columns=['species', 'audio_files'])\n",
    "audio_df = audio_df.explode('audio_files').reset_index(drop=True)\n",
    "\n",
    "audio_df = audio_df.apply(mfcc_extractor, axis=1, chunk_size=2)\n",
    "audio_df_exploded = audio_df.explode('chunk_list').reset_index(drop=True)\n",
    "\n",
    "print(f'Different species in audio dataset: {audio_df_exploded[\"species\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c05948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Different species in audio dataset: {audio_df_exploded[\"species\"].unique()}')\n",
    "print(audio_df_exploded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e778cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminare le righe con liste vuote in 'chunk_list'\n",
    "audio_df_exploded_clean = audio_df_exploded.dropna(axis=0, subset=['chunk_list'])\n",
    "valid_chunks = audio_df_exploded_clean['chunk_list']\n",
    "valid_chunks = valid_chunks[valid_chunks.apply(lambda x: isinstance(x, np.ndarray) and len(x) == 50)]\n",
    "\n",
    "# Converti in matrice\n",
    "mfcc_matrix = np.array(valid_chunks.tolist())\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "    \n",
    "x_transformed = tsne.fit_transform(mfcc_matrix)\n",
    "tsne_df = pd.DataFrame(np.column_stack((x_transformed, audio_df_exploded_clean[\"species\"])), columns=['X', 'Y', \"Targets\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df.loc[:, \"Targets\"] = tsne_df.Targets.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec328e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "g = sns.FacetGrid(data=tsne_df, hue='Targets', height=8, palette=\"tab10\")\n",
    "g.map(plt.scatter, 'X', 'Y').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe20307",
   "metadata": {},
   "source": [
    "## Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_CACHE_DIR = os.path.join(\"data_cache\", \"CNN\")\n",
    "MODELS_METRICS_DIR = os.path.join(\"models_metrics\")\n",
    "PATIENCE = 3\n",
    "FROM_START = False\n",
    "if not os.path.exists(MODELS_METRICS_DIR):\n",
    "    os.makedirs(MODELS_METRICS_DIR)\n",
    "\n",
    "split_perc = {'train': 0.8, 'val': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca453a",
   "metadata": {},
   "source": [
    "Training della CNN classica con le classi che contengono piÃ¹ di 1000 sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ec4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_classes_1000 = train_routine(count_df, PATIENCE, SPLIT_PERC, DATA_DIR, (w, h), (0, 0), subfloder='22-09_training_01', to_train=False, cardinality=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba605c51",
   "metadata": {},
   "source": [
    "Si ripete il training aggiungendo 10 classi per volta in ordine decrescente in numero di sample contenuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fino a classe 23\n",
    "n_classes_plus_10 = train_routine(count_df, PATIENCE, SPLIT_PERC, DATA_DIR, (w, h), (n_classes_1000, 10), subfloder='22-09_training_02', to_train=TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fino a classe 33\n",
    "n_classes_plus_20 = train_routine(count_df, PATIENCE, SPLIT_PERC, DATA_DIR, (w, h), (n_classes_plus_10, 10), subfloder='22-09_training_03', to_train=TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fino a classe 43\n",
    "n_classes_plus_30 = train_routine(count_df, PATIENCE, SPLIT_PERC, DATA_DIR, (w, h), (n_classes_plus_20, 10), subfloder='22-09_training_04', to_train=TO_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fino all'ultima classe\n",
    "n_classes_plus_rem = train_routine(count_df, PATIENCE, SPLIT_PERC, DATA_DIR, (w, h), (n_classes_plus_30, 10), subfloder='22-09_training_05', to_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a359d3",
   "metadata": {},
   "source": [
    "## Output Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(cm, label_dict):\n",
    "    num_classes = cm.shape[0]\n",
    "    true_positives = np.diag(cm)\n",
    "    false_positives = np.sum(cm, axis=0) - true_positives\n",
    "    false_negatives = np.sum(cm, axis=1) - true_positives\n",
    "\n",
    "    support =  np.asarray([label_dict[i][\"support\"] for i in range(num_classes)], dtype=int)\n",
    "    true_negatives = np.sum(cm) - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    precision = np.divide(true_positives, true_positives + false_positives, out=np.zeros_like(true_positives, dtype=float), where=(true_positives + false_positives) != 0)\n",
    "    recall = np.divide(true_positives, true_positives + false_negatives, out=np.zeros_like(true_positives, dtype=float), where=(true_positives + false_negatives) != 0)\n",
    "    f1_score = np.divide(2 * precision * recall, precision + recall, out=np.zeros_like(precision, dtype=float), where=(precision + recall) != 0)\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'label': [label_dict[i][\"label\"] for i in range(num_classes)],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1_score,\n",
    "        'support': support,\n",
    "        'tp': true_positives,\n",
    "        'fp': false_positives,\n",
    "        'fn': false_negatives,\n",
    "        'tn': true_negatives\n",
    "    })\n",
    "\n",
    "    metrics_df = metrics_df.sort_values(by='support', ascending=False).reset_index(drop=True)\n",
    "    return metrics_df\n",
    "\n",
    "def process_metrics(n_classes, training_date):\n",
    "    all_classes_df = pd.read_csv(os.path.join(MODELS_METRICS_DIR, f'{training_date}_training', f'{n_classes}_training_log.csv'))\n",
    "    label_df = pd.read_csv(os.path.join(MODELS_METRICS_DIR, f'{training_date}_training', f'{n_classes}_label_to_index.csv'))\n",
    "    label_df = label_df.merge(count_df, left_on='label', right_on='species', how='left', validate='one_to_one', suffixes=('_training', '_total')).drop(columns=['species'])\n",
    "    label_df['support'] = label_df['file_count_total'] - label_df['file_count_training']\n",
    "\n",
    "    best_weights = all_classes_df[all_classes_df['val_accuracy'] == all_classes_df['val_accuracy'].max()]\n",
    "    cm = best_weights['val_confusion_matrix']\n",
    "    cm = cm.values[0]\n",
    "    cm = cm[2:-2]\n",
    "    cm_list = cm.split(', ')\n",
    "    cm_matrix = []\n",
    "    for r in cm_list:\n",
    "        r = r[1:-1]\n",
    "        r = r.split()\n",
    "        cm_matrix.append([int(i) for i in r])\n",
    "    cm_matrix = np.array(cm_matrix)\n",
    "    label_dict = label_df.to_dict('index')\n",
    "    metrics_df = get_metrics(cm_matrix, label_dict)\n",
    "    metrics_df.to_csv(os.path.join(MODELS_METRICS_DIR, f'{training_date}_training', f'{n_classes}_metrics.csv'), index=False)\n",
    "    metrics_plot_builder(metrics_df)\n",
    "    confusion_matrix_plot(cm_matrix, metrics_df['label'].tolist())\n",
    "\n",
    "\n",
    "def confusion_matrix_plot(cm, labels):\n",
    "    cmn = cm.astype('int') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    def custom_format(val):\n",
    "        if val < 0.01:\n",
    "            return \"0\"\n",
    "        else:\n",
    "            return f\"{val:.2f}\"\n",
    "    formatted_annotations = np.vectorize(custom_format)(cmn)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(cmn, annot=formatted_annotations, fmt='', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def metrics_plot_builder(metrics_df):\n",
    "    metrics_list = ['precision', 'recall', 'f1-score']\n",
    "    f = 1\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    for metric in metrics_list:\n",
    "        axs = plt.subplot(2, 2, f)\n",
    "        axs.bar(metrics_df['label'], metrics_df[metric], color=\"#87CEEB\")\n",
    "        for i, (metric_value, support) in enumerate(zip(metrics_df[metric], metrics_df['support'])):\n",
    "            label_pos = metric_value - (metric_value/2) if metric_value > 0 else metric_value + 0.02\n",
    "            plt.text(i, label_pos, f'n:{int(support)}', ha='center', va='bottom', fontsize=9, rotation=90)\n",
    "        axs.set_xlabel('Class')\n",
    "        axs.tick_params(axis='x', rotation=90)\n",
    "        axs.set_ylabel(metric.capitalize())\n",
    "        axs.set_title(f'{metric.capitalize()} per Class (ordered by Support - descending)')\n",
    "        \n",
    "        f += 1\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_training_date = '21-09'\n",
    "process_metrics(n_classes_1000, curr_training_date)\n",
    "process_metrics(n_classes_plus_10, curr_training_date)\n",
    "process_metrics(n_classes_plus_20, curr_training_date)\n",
    "process_metrics(n_classes_plus_30, curr_training_date)\n",
    "process_metrics(n_classes_plus_rem, curr_training_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97744a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mmd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
